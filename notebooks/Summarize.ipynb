{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6rko_ANX0EC"
   },
   "source": [
    "# Document Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook demonstrates an application of long document summarization techniques to a work of literature using Granite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you are running python 3.10, 3.11, or 3.12 in a freshly-created virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 10) and sys.version_info < (\n",
    "    3,\n",
    "    13,\n",
    "), \"Use Python 3.10, 3.11, or 3.12 to run this notebook.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To format the cells\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving the Granite AI model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook requires IBM Granite models to be served by an AI model runtime so that the models can be invoked or called. This notebook can use a locally accessible [Ollama](https://github.com/ollama/ollama) server to serve the models, or the [Replicate](https://replicate.com) cloud service.\n",
    "\n",
    "During the pre-work, you may have either started a local Ollama server on your computer, or setup Replicate access and obtained an [API token](https://replicate.com/account/api-tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydrVWz7EYHh9"
   },
   "source": [
    "## Select your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a Granite model to use. Here we use a Langchain client to connect to the model. If there is a locally accessible Ollama server, we use an Ollama client to access the model. Otherwise, we use a Replicate client to access the model.\n",
    "\n",
    "When using Replicate, if the `REPLICATE_API_TOKEN` environment variable is not set, or a `REPLICATE_API_TOKEN` Colab secret is not set, then the notebook will ask for your [Replicate API token](https://replicate.com/account/api-tokens) in a dialog box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TSkiGBY4qo32"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_community.llms import Replicate\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "\n",
    "try:  # Look for a locally accessible Ollama server for the model\n",
    "    response = requests.get(os.getenv(\"OLLAMA_HOST\", \"http://127.0.0.1:11434\"))\n",
    "    model = OllamaLLM(\n",
    "        model=\"granite3.2:2b\",\n",
    "        num_ctx=65536,  # 64K context window\n",
    "    )\n",
    "    model = model.bind(raw=True)  # Client side controls prompt\n",
    "except Exception:  # Use Replicate for the model\n",
    "    model = Replicate(\n",
    "        model=\"ibm-granite/granite-3.2-8b-instruct\",\n",
    "        replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"),\n",
    "        model_kwargs={\n",
    "            \"max_tokens\": 2000,  # Set the maximum number of tokens to generate as output.\n",
    "            \"min_tokens\": 200,  # Set the minimum number of tokens to generate as output.\n",
    "            \"temperature\": 0.75,\n",
    "            \"presence_penalty\": 0,\n",
    "            \"frequency_penalty\": 0,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d0sWaZ7YLHN"
   },
   "source": [
    "## Download a book\n",
    "\n",
    "Here we fetch H.D. Thoreau's \"Walden\" from [Project Gutenberg](https://www.gutenberg.org/) for summarization.\n",
    "\n",
    "We have to chunk the book text so that chunks will fit in the context window size of the AI model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYuQmgRJY0n5"
   },
   "source": [
    "### Count the tokens\n",
    "\n",
    "Before sending our book chunks to the AI model, it's crucial to understand how much of the model's capacity we're using. Language models typically have a limit on the number of tokens they can process in a single request.\n",
    "\n",
    "Key points:\n",
    "- We're using the [`granite-3.2`](https://huggingface.co/ibm-granite/granite-3.1-2b-instruct) model, which has a context window of 128K tokens.\n",
    "- Tokenization can vary between models, so we use the specific tokenizer for our chosen model.\n",
    "\n",
    "Understanding token count helps us optimize our prompts and ensure we're using the model efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7JqmvTqbWPgl"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"ibm-granite/granite-3.2-8b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Summaries\n",
    "\n",
    "Here we use a hierarchical abstractive summarization technique to adapt to the context length of the model. Our approach uses [Docling](https://docling-project.github.io/docling/) to understand the document's structure, chunk the document into text passages, and group the text passages by chapter which we can then summarize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 documents created\n",
      "Max document size: 38275 tokens\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from typing import Iterator, Callable\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling_core.transforms.chunker.hierarchical_chunker import HierarchicalChunker\n",
    "from docling_core.transforms.chunker.base import BaseChunk\n",
    "\n",
    "\n",
    "def chunk_document(\n",
    "    source: str,\n",
    "    *,\n",
    "    dropwhile: Callable[[BaseChunk], bool] = lambda c: False,\n",
    "    takewhile: Callable[[BaseChunk], bool] = lambda c: True,\n",
    ") -> Iterator[BaseChunk]:\n",
    "    \"\"\"Read the document and perform a hierarchical chunking\"\"\"\n",
    "    converter = DocumentConverter()\n",
    "    chunks = HierarchicalChunker().chunk(converter.convert(source=source).document)\n",
    "    return itertools.takewhile(takewhile, itertools.dropwhile(dropwhile, chunks))\n",
    "\n",
    "\n",
    "def merge_chunks(\n",
    "    chunks: Iterator[BaseChunk],\n",
    "    *,\n",
    "    headings: Callable[[BaseChunk], list[str]] = lambda c: c.meta.headings,\n",
    ") -> Iterator[dict[str, str]]:\n",
    "    \"\"\"Merge chunks having the same headings\"\"\"\n",
    "    prior_headings: list[str] | None = None\n",
    "    document: dict[str, str] = {}\n",
    "    for chunk in chunks:\n",
    "        text = chunk.text.replace(\"\\r\\n\", \"\\n\")\n",
    "        current_headings = headings(chunk)\n",
    "        if prior_headings != current_headings:\n",
    "            if document:\n",
    "                yield document\n",
    "            prior_headings = current_headings\n",
    "            document = {\"title\": \" - \".join(current_headings), \"text\": text}\n",
    "        else:\n",
    "            document[\"text\"] += f\"\\n\\n{text}\"\n",
    "    if document:\n",
    "        yield document\n",
    "\n",
    "\n",
    "def chunk_dropwhile(chunk: BaseChunk) -> bool:\n",
    "    \"\"\"Ignore front matter prior to the book start\"\"\"\n",
    "    return \"WALDEN\" not in chunk.meta.headings\n",
    "\n",
    "\n",
    "def chunk_takewhile(chunk: BaseChunk) -> bool:\n",
    "    \"\"\"Ignore remaining chunks once we see this heading\"\"\"\n",
    "    return \"ON THE DUTY OF CIVIL DISOBEDIENCE\" not in chunk.meta.headings\n",
    "\n",
    "\n",
    "def chunk_headings(chunk: BaseChunk) -> list[str]:\n",
    "    \"\"\"Use the h1 and h2 (chapter) headings\"\"\"\n",
    "    return chunk.meta.headings[:2]\n",
    "\n",
    "\n",
    "documents: list[dict[str, str]] = list(\n",
    "    merge_chunks(\n",
    "        chunk_document(\n",
    "            \"https://www.gutenberg.org/cache/epub/205/pg205-images.html\",\n",
    "            dropwhile=chunk_dropwhile,\n",
    "            takewhile=chunk_takewhile,\n",
    "        ),\n",
    "        headings=chunk_headings,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"{len(documents)} documents created\")\n",
    "print(\n",
    "    f\"Max document size: {max(len(tokenizer.tokenize(document['text'])) for document in documents)} tokens\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the chunks\n",
    "\n",
    "Here we define a method to generate a response using a list of documents and a user prompt about those documents. \n",
    "\n",
    "We create the prompt according to the [Granite Prompting Guide](https://www.ibm.com/granite/docs/models/granite/#chat-template) and provide the documents using the `documents` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(user_prompt: str, documents: list[dict[str, str]]):\n",
    "    \"\"\"Use the chat template to format the prompt\"\"\"\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        conversation=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt,\n",
    "            }\n",
    "        ],\n",
    "        documents=documents,  # This uses the documents support in the Granite chat template\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "\n",
    "    print(f\"Input size: {len(tokenizer.tokenize(prompt))} tokens\")\n",
    "    output = model.invoke(prompt)\n",
    "    print(f\"Output size: {len(tokenizer.tokenize(output))} tokens\")\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each chapter, we create a separate summary. This can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= WALDEN - Economy =============================\n",
      "Input size: 38418 tokens\n",
      "Output size: 348 tokens\n",
      "============================= WALDEN - Where I Lived, and What I Lived For =============================\n",
      "Input size: 9133 tokens\n",
      "Output size: 348 tokens\n",
      "============================= WALDEN - Reading =============================\n",
      "Input size: 5719 tokens\n",
      "Output size: 170 tokens\n",
      "============================= WALDEN - Sounds =============================\n",
      "Input size: 9102 tokens\n",
      "Output size: 237 tokens\n",
      "============================= WALDEN - Solitude =============================\n",
      "Input size: 5214 tokens\n",
      "Output size: 370 tokens\n",
      "============================= WALDEN - Visitors =============================\n",
      "Input size: 7148 tokens\n",
      "Output size: 191 tokens\n",
      "============================= WALDEN - The Bean-Field =============================\n",
      "Input size: 6332 tokens\n",
      "Output size: 312 tokens\n",
      "============================= WALDEN - The Village =============================\n",
      "Input size: 3133 tokens\n",
      "Output size: 255 tokens\n",
      "============================= WALDEN - The Ponds =============================\n",
      "Input size: 13841 tokens\n",
      "Output size: 195 tokens\n",
      "============================= WALDEN - Baker Farm =============================\n",
      "Input size: 4156 tokens\n",
      "Output size: 192 tokens\n",
      "============================= WALDEN - Higher Laws =============================\n",
      "Input size: 6463 tokens\n",
      "Output size: 398 tokens\n",
      "============================= WALDEN - Brute Neighbors =============================\n",
      "Input size: 7278 tokens\n",
      "Output size: 155 tokens\n",
      "============================= WALDEN - House-Warming =============================\n",
      "Input size: 8665 tokens\n",
      "Output size: 254 tokens\n",
      "============================= WALDEN - Former Inhabitants and Winter Visitors =============================\n",
      "Input size: 7546 tokens\n",
      "Output size: 265 tokens\n",
      "============================= WALDEN - Winter Animals =============================\n",
      "Input size: 5751 tokens\n",
      "Output size: 243 tokens\n",
      "============================= WALDEN - The Pond in Winter =============================\n",
      "Input size: 7783 tokens\n",
      "Output size: 182 tokens\n",
      "============================= WALDEN - Spring =============================\n",
      "Input size: 10407 tokens\n",
      "Output size: 458 tokens\n",
      "============================= WALDEN - Conclusion =============================\n",
      "Input size: 7047 tokens\n",
      "Output size: 293 tokens\n",
      "Summary count: 18\n"
     ]
    }
   ],
   "source": [
    "if get_env_var(\"GRANITE_TESTING\", \"false\").lower() == \"true\":\n",
    "    documents = documents[:5]  # shorten testing work\n",
    "\n",
    "user_prompt = \"\"\"\\\n",
    "Using only the the book chapter document, compose a summary of the book chapter.\n",
    "Your response should only include the summary. Do not provide any further explanation.\"\"\"\n",
    "\n",
    "summaries: list[dict[str, str]] = []\n",
    "\n",
    "for document in documents:\n",
    "    print(\n",
    "        f\"============================= {document['title']} =============================\"\n",
    "    )\n",
    "    output = generate(user_prompt, [document])\n",
    "    summaries.append({\"title\": document[\"title\"], \"text\": output})\n",
    "\n",
    "print(\"Summary count: \" + str(len(summaries)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Final Summary\n",
    "\n",
    "Now we need to summarize the chapter summaries. We prompt the model to create a unified summary of the chapter summaries we previously generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: 5122 tokens\n",
      "Output size: 733 tokens\n",
      "\"Walden\" by Henry David Thoreau is a reflection on simple living and self-reliance, set near Walden Pond in Concord, Massachusetts. Thoreau recounts his experiences in a small, self-built cabin, advocating for a life free from materialism and social conventions. He praises the beauty and tranquility of nature, contrasting it with the hurried, materialistic lifestyle of his contemporaries. Thoreau critiques the excesses of civilized life and the pursuit of wealth, arguing that most people lead lives of \"quiet desperation.\" Instead, he encourages readers to embrace voluntary simplicity, self-sufficiency, and a deeper understanding of the world through personal experience and direct observation of nature.\n",
      "\n",
      "Thoreau details his modest lifestyle, including a simple diet, minimal furniture, and inexpensive clothing, and shares his experiences in various trades, enabling him to work only about six weeks a year. He questions the value of conventional education and societal norms, suggesting that true knowledge comes from personal experience. The book is filled with poetic descriptions of nature and philosophical musings on life, encouraging mindfulness, understanding reality, and finding fulfillment in simple pleasures.\n",
      "\n",
      "Thoreau also reflects on solitude, finding comfort and companionship in nature, and appreciating the peacefulness of his isolated life. He values deep, meaningful thoughts over fleeting social interactions and believes in the importance of self-reliance and inner strength. Thoreau occasionally receives visitors but finds his primary companionship in the natural world, which he views as innocent, benevolent, and constantly sympathetic to humanity.\n",
      "\n",
      "The text also includes chapters on Thoreau's experiences with hunting, fishing, and farming, emphasizing the intimate connection with nature and the land. He muses on the symbolic value of these activities and the cultivation of virtues like sincerity, truth, and simplicity. Thoreau discusses his personal evolution regarding these activities and his thoughts on sensuality, purity, and the human condition. He argues for a more innocent and wholesome diet as part of humanity's future improvement.\n",
      "\n",
      "Thoreau's chapters often describe his daily routine, interactions with visitors, and observations of wildlife. He reflects on the behavior of moles in his cellar, the arrival of various wildlife as seasons change, and the impact of winter on local fauna. He also explores the universality of water and the interconnectedness of all things.\n",
      "\n",
      "Throughout \"Walden,\" Thoreau emphasizes the importance of deep, thoughtful reading, particularly of classic literature, and advocates for a higher standard of education. He calls for a shift away from provincial thinking and a greater engagement with the world's knowledge and intellectual resources.\n",
      "\n",
      "In essence, \"Walden\" is a manifesto for voluntary simplicity, self-sufficiency, and a critique of the materialism and social conventions of 19th-century American society. Thoreau encourages readers to question their assumptions about the necessities of life, live more authentically, and independently.\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"\\\n",
    "Using only the book chapter summary documents, compose a single, unified summary of the book.\n",
    "Your response should only include the unified summary. Do not provide any further explanation.\"\"\"\n",
    "\n",
    "output = generate(user_prompt, summaries)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have now summarized a document larger than the AI model's context window length by breaking the document down into smaller pieces to summarize and then summarizing those summaries."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
