
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Retrieval Augmented Generation (RAG) with Langchain">
      
      
        <meta name="author" content="IBM Research">
      
      
        <link rel="canonical" href="https://widsireland.github.io/WiDSIrelandLab2025/lab-2/readme/">
      
      
        <link rel="prev" href="../../lab-1/readme/">
      
      
        <link rel="next" href="../../lab-3/readme/">
      
      
      <link rel="icon" href="../../images/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Women in Data Science Workshop Lab 2 - WiDS Ireland Lab 2025</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=IBM+Plex+Sans:300,300i,400,400i,700,700i%7CIBM+Plex+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"IBM Plex Sans";--md-code-font:"IBM Plex Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#retrieval-augmented-generation-rag-with-langchain" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="WiDS Ireland Lab 2025" class="md-header__button md-logo" aria-label="WiDS Ireland Lab 2025" data-md-component="logo">
      
  <img src="../../images/ibm-blue-background.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            WiDS Ireland Lab 2025
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Women in Data Science Workshop Lab 2
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/WiDSIreland/WiDSIrelandLab2025" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    WiDSIrelandLab2025
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../docs/index.md" class="md-tabs__link">
          
  
  
  Welcome

        </a>
      </li>
    
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../pre-work/readme/" class="md-tabs__link">
          
  
  
  Workshop

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="https://github.com/ibm-granite-community" class="md-tabs__link">
          
  
  
  References

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="WiDS Ireland Lab 2025" class="md-nav__button md-logo" aria-label="WiDS Ireland Lab 2025" data-md-component="logo">
      
  <img src="../../images/ibm-blue-background.png" alt="logo">

    </a>
    WiDS Ireland Lab 2025
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/WiDSIreland/WiDSIrelandLab2025" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    WiDSIrelandLab2025
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Welcome
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Welcome
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../docs/index.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    About the workshop
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Workshop
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Workshop
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pre-work/readme/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Lab 0. Pre-work
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../lab-1/readme/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Lab 1. Summarize a text document with Granite
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Lab 2. Retrieval Augmented Generation (RAG) with Langchain
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Lab 2. Retrieval Augmented Generation (RAG) with Langchain
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#prerequisites" class="md-nav__link">
    <span class="md-ellipsis">
      Prerequisites
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#loading-the-lab" class="md-nav__link">
    <span class="md-ellipsis">
      Loading the Lab
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#running-and-lab-with-explanations" class="md-nav__link">
    <span class="md-ellipsis">
      Running and Lab (with explanations)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#choosing-the-embeddings-model" class="md-nav__link">
    <span class="md-ellipsis">
      Choosing the Embeddings Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#choose-your-vector-database" class="md-nav__link">
    <span class="md-ellipsis">
      Choose your Vector Database
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#selecting-your-model" class="md-nav__link">
    <span class="md-ellipsis">
      Selecting your model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#building-the-vector-database" class="md-nav__link">
    <span class="md-ellipsis">
      Building the Vector Database
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Building the Vector Database">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#download-the-document" class="md-nav__link">
    <span class="md-ellipsis">
      Download the document
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../lab-3/readme/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Lab 3. Energy Demand Forecasting with Granite Timeseries (TTM)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    References
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            References
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://github.com/ibm-granite-community" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    IBM Granite Community
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#prerequisites" class="md-nav__link">
    <span class="md-ellipsis">
      Prerequisites
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#loading-the-lab" class="md-nav__link">
    <span class="md-ellipsis">
      Loading the Lab
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#running-and-lab-with-explanations" class="md-nav__link">
    <span class="md-ellipsis">
      Running and Lab (with explanations)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#choosing-the-embeddings-model" class="md-nav__link">
    <span class="md-ellipsis">
      Choosing the Embeddings Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#choose-your-vector-database" class="md-nav__link">
    <span class="md-ellipsis">
      Choose your Vector Database
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#selecting-your-model" class="md-nav__link">
    <span class="md-ellipsis">
      Selecting your model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#building-the-vector-database" class="md-nav__link">
    <span class="md-ellipsis">
      Building the Vector Database
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Building the Vector Database">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#download-the-document" class="md-nav__link">
    <span class="md-ellipsis">
      Download the document
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="retrieval-augmented-generation-rag-with-langchain">Retrieval Augmented Generation (RAG) with Langchain<a class="headerlink" href="#retrieval-augmented-generation-rag-with-langchain" title="Permanent link">&para;</a></h1>
<p><a href="https://research.ibm.com/blog/retrieval-augmented-generation-RAG">Retrieval Augumented Generation (RAG)</a> is an architectural pattern that can be used to augment the performance of language models by recalling factual information from a knowledge base, and adding that information to the model query.</p>
<p>The goal of this lab is to show how you can use RAG with an <a href="https://www.ibm.com/granite">IBM Granite</a> model to augment the model query answer using a publicly available document. The most common approach in RAG is to create dense vector representations of the knowledge base in order to retrieve text chunks that are semantically similar to a given user query.</p>
<p>RAG use cases include:
- Customer service: Answering questions about a product or service using facts from the product documentation.
- Domain knowledge: Exploring a specialized domain (e.g., finance) using facts from papers or articles in the knowledge base.
- News chat: Chatting about current events by calling up relevant recent news articles.</p>
<p>In its simplest form, RAG requires 3 steps:</p>
<ul>
<li>Initial setup:</li>
<li>Index knowledge-base passages for efficient retrieval. In this recipe, we take embeddings of the passages and store them in a vector database.</li>
<li>Upon each user query:</li>
<li>Retrieve relevant passages from the database. In this recipe, we use an embedding of the query to retrieve semantically similar passages.</li>
<li>Generate a response by feeding retrieved passage into a large language model, along with the user query.</li>
</ul>
<h2 id="prerequisites">Prerequisites<a class="headerlink" href="#prerequisites" title="Permanent link">&para;</a></h2>
<p>This lab is a <a href="https://jupyter.org/">Jupyter notebook</a>. Please follow the instructions in <a href="../../pre-work/readme/">pre-work</a> to run the lab.</p>
<h2 id="loading-the-lab">Loading the Lab<a class="headerlink" href="#loading-the-lab" title="Permanent link">&para;</a></h2>
<p>Using colab to run the remotely <a href="{{ extra.colab_url }}/blob/{{ git.commit }}/notebooks/RAG_with_Langchain.ipynb" target="_blank"><img alt="Document Summarization with Granite notebook" src="https://colab.research.google.com/assets/colab-badge.svg" title="Open In Colab" /></a></p>
<p>To run the notebook from your command line in Jupyter using the active virtual environment from the <a href="../../pre-work/readme/">pre-work</a>, run:</p>
<div class="highlight"><pre><span></span><code>jupyter-lab
</code></pre></div>
<p>When Jupyter Lab opens the path to the <code>notebooks/RAG_with_Langchain.ipynb</code> notebook file is relative to the <code>sample-wids</code> folder from the git clone in the <a href="../../pre-work/readme/">pre-work</a>. The folder navigation pane on the left-hand side can be used to navigate to the file. Once the notebook has been found it can be double clicked and it will open to the pane on the right. </p>
<h2 id="running-and-lab-with-explanations">Running and Lab (with explanations)<a class="headerlink" href="#running-and-lab-with-explanations" title="Permanent link">&para;</a></h2>
<p>This notebook demonstrates an application of long document summarisation techniques to a work of literature using Granite.</p>
<p>The notebook contains both <code>code</code> cells and <code>markdown</code> text cells. The text cells each give a brief overview of the code in the following code cell(s). These cells are not executable. You can execute the code cells by placing your cursor in the cell and then either hitting the <strong>Run this cell</strong> button at the top of the page or by pressing the <code>Shift</code> + <code>Enter</code> keys together. The main <code>code</code> cells are described in detail below.</p>
<h2 id="choosing-the-embeddings-model">Choosing the Embeddings Model<a class="headerlink" href="#choosing-the-embeddings-model" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">embeddings_model_path</span> <span class="o">=</span> <span class="s2">&quot;ibm-granite/granite-embedding-30m-english&quot;</span>
<span class="n">embeddings_model</span> <span class="o">=</span> <span class="n">HuggingFaceEmbeddings</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="n">embeddings_model_path</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">embeddings_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">embeddings_model_path</span><span class="p">)</span>
</code></pre></div>
<p>Here we are using the Hugging Face Transformers library to load a pre-trained model for generating embeddings (vector representations of text). Here's a breakdown of what each line does:</p>
<ol>
<li>
<p><code>from langchain_huggingface import HuggingFaceEmbeddings</code>: This line imports the <code>HuggingFaceEmbeddings</code> class from the <code>langchain_huggingface</code> module. This class is used to load pre-trained models for generating embeddings.</p>
</li>
<li>
<p><code>from transformers import AutoTokenizer</code>: This line imports the <code>AutoTokenizer</code> class from the <code>transformers</code> library. This class is used to tokenize text into smaller pieces (words, subwords, etc.) that can be processed by the model.</p>
</li>
<li>
<p><code>embeddings_model_path = "ibm-granite/granite-embedding-30m-english"</code> : This line sets a variable <code>embeddings_model_path</code> to the path of the pre-trained model. In this case, it's a model called "granite-embedding-30m-english" developed by IBM's Granite project.</p>
</li>
<li>
<p><code>embeddings_model = HuggingFaceEmbeddings(model_name=embeddings_model_path)</code>: This line creates an instance of the <code>HuggingFaceEmbeddings</code> class, loading the pre-trained model specified by <code>embeddings_model_path</code>.</p>
</li>
<li>
<p><code>embeddings_tokenizer = AutoTokenizer.from_pretrained(embeddings_model_path)</code>: This line creates an instance of the <code>AutoTokenizer</code> class, loading the tokenizer that was trained alongside the specified model. This tokenizer will be used to convert text into a format that the model can process.</p>
</li>
</ol>
<p>In summary, we are setting up a system for generating embeddings from text using a pre-trained model and its associated tokenizer. The embeddings can then be used for various natural language processing tasks, such as text classification, clustering, or similarity comparison.</p>
<p>To use a model from a provider other than Huggingface, replace this code cell with one from <a href="https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_Embeddings_Models.ipynb">this Embeddings Model recipe</a>.</p>
<h2 id="choose-your-vector-database">Choose your Vector Database<a class="headerlink" href="#choose-your-vector-database" title="Permanent link">&para;</a></h2>
<p>Specify the database to use for storing and retrieving embedding vectors.</p>
<p>To connect to a vector database other than Milvus substitute this code cell with one from <a href="https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_Vector_Stores.ipynb">this Vector Store recipe</a>.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_milvus</span><span class="w"> </span><span class="kn">import</span> <span class="n">Milvus</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>

<span class="n">db_file</span> <span class="o">=</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;milvus_&quot;</span><span class="p">,</span> <span class="n">suffix</span><span class="o">=</span><span class="s2">&quot;.db&quot;</span><span class="p">,</span> <span class="n">delete</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">name</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The vector database will be saved to </span><span class="si">{</span><span class="n">db_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">vector_db</span> <span class="o">=</span> <span class="n">Milvus</span><span class="p">(</span>
    <span class="n">embedding_function</span><span class="o">=</span><span class="n">embeddings_model</span><span class="p">,</span>
    <span class="n">connection_args</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;uri&quot;</span><span class="p">:</span> <span class="n">db_file</span><span class="p">},</span>
    <span class="n">auto_id</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">index_params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;index_type&quot;</span><span class="p">:</span> <span class="s2">&quot;AUTOINDEX&quot;</span><span class="p">},</span>
<span class="p">)</span>
</code></pre></div>
<p>This Python script is setting up a vector database using Milvus, a vector database built for AI applications, and Hugging Face's Transformers library for embeddings. It uses the previously created Embeddings Model. Here's a breakdown of what the code does:</p>
<ol>
<li>It imports <code>tempfile</code> and <code>Milvus</code> from <code>langchain_milvus</code>.</li>
<li>It creates a temporary file for the Milvus database using <code>tempfile.NamedTemporaryFile()</code>. This file will store the vector database.</li>
<li>It initializes an instance of <code>Milvus</code>with the embedding function set to the previously created <code>embeddings_model</code>. The connection arguments specify the URI of the database file, which is the temporary file created in the previous step. The <code>auto_id</code> parameter is set to True, which means Milvus will automatically generate IDs for the vectors. The <code>index_params</code> parameter sets the index type to "AUTOINDEX", which allows Milvus to automatically choose the most suitable index for the data.</li>
</ol>
<p>In summary, this script sets up a vector database using Milvus and a pre-trained embedding model from Hugging Face. The database is stored in a temporary file, and it's ready to index and search vector representations of text data.</p>
<h2 id="selecting-your-model">Selecting your model<a class="headerlink" href="#selecting-your-model" title="Permanent link">&para;</a></h2>
<p>Select a Granite model to use. Here we use a Langchain client to connect to  the model. If there is a locally accessible Ollama server, we use an  Ollama client to access the model. Otherwise, we use a Replicate client to access the model.</p>
<p>When using Replicate, if the <code>REPLICATE_API_TOKEN</code> environment variable is not set, or a <code>REPLICATE_API_TOKEN</code> Colab secret is not set, then the notebook will ask for your <a href="https://replicate.com/account/api-tokens">Replicate API token</a> in a dialog box.</p>
<p><div class="highlight"><pre><span></span><code><span class="n">model_path</span> <span class="o">=</span> <span class="s2">&quot;ibm-granite/granite-3.2-8b-instruct&quot;</span>
<span class="k">try</span><span class="p">:</span>  <span class="c1"># Look for a locally accessible Ollama server for the model</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;OLLAMA_HOST&quot;</span><span class="p">,</span> <span class="s2">&quot;http://127.0.0.1:11434&quot;</span><span class="p">))</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">OllamaLLM</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;granite3.2:2b&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">raw</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Client side controls prompt</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>  <span class="c1"># Use Replicate for the model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Replicate</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model_path</span><span class="p">,</span>
        <span class="n">replicate_api_token</span><span class="o">=</span><span class="n">get_env_var</span><span class="p">(</span><span class="s2">&quot;REPLICATE_API_TOKEN&quot;</span><span class="p">),</span>
    <span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
</code></pre></div>
1. <code>model_path = "ibm-granite/granite-3.2-8b-instruct"</code>: This line assigns the string <code>"ibm-granite/granite-3.2-8b-instruct"</code> to the <code>model_path</code> variable. This is the name of the pre-trained model on the Hugging Face Model Hub that will be used for the language model.
2. <code>try:</code>: This line starts a try block, which is used to handle exceptions that may occur during the execution of the code within the block.
3. <code>response = requests.get(os.getenv("OLLAMA_HOST", "http://127.0.0.1:11434"))</code>: This line sends a GET request to the Ollama server using the <code>requests.get()</code> function. The server address is obtained from the <code>OLLAMA_HOST</code> environment variable. If the environment variable is not set, the default address <code>http://127.0.0.1:11434</code> is used.
4. <code>model = OllamaLLM(model="granite3.2:2b")</code>: This line creates an instance of the <code>OllamaLLM</code> class from the <code>ollama</code> library, specifying the model name as <code>"granite3.2:2b"</code>.
5. <code>model = model.bind(raw=True)</code>: This line binds the <code>OllamaLLM</code> instance to the client-side, allowing client-side controls over the prompt.
6. <code>except:</code>: This line starts an except block, which is used to handle exceptions that occur within the try block.
7. <code>model = Replicate(model=model_path, replicate_api_token=get_env_var("REPLICATE_API_TOKEN"))</code>: This line creates an instance of the <code>Replicate</code> class from the <code>replicate</code> library, specifying the model path and the Replicate API token obtained from the <code>REPLICATE_API_TOKEN</code> environment variable.
8. <code>tokenizer = AutoTokenizer.from_pretrained(model_path)</code>: This line loads a pre-trained tokenizer for the specified model using the <code>AutoTokenizer.from_pretrained()</code> method from the <code>transformers</code> library.</p>
<p>In summary, the code snippet attempts to connect to a locally accessible Ollama server for the specified model. If the connection is successful, it creates an <code>OllamaLLM</code> instance and binds it to the client-side. If the connection fails, it uses the Replicate service to load the model. In both cases, a tokenizer is loaded for the specified model using the <code>AutoTokenizer.from_pretrained()</code> method.</p>
<h2 id="building-the-vector-database">Building the Vector Database<a class="headerlink" href="#building-the-vector-database" title="Permanent link">&para;</a></h2>
<p>In this example, we take the State of the Union speech text, split it into chunks, derive embedding vectors using the embedding model, and load it into the vector database for querying.</p>
<h3 id="download-the-document">Download the document<a class="headerlink" href="#download-the-document" title="Permanent link">&para;</a></h3>
<p>Here we use President Biden's State of the Union address from March 1, 2022.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">wget</span>

<span class="n">filename</span> <span class="o">=</span> <span class="s2">&quot;state_of_the_union.txt&quot;</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/data/foundation_models/state_of_the_union.txt&quot;</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="n">wget</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">filename</span><span class="p">)</span>
</code></pre></div>
<ol>
<li><code>filename = "state_of_the_union.txt"</code>: This line assigns the string <code>"state_of_the_union.txt"</code> to the <code>filename</code> variable. This is the name of the file that will be downloaded and saved locally.</li>
<li><code>url = "https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/data/foundation_models/state_of_the_union.txt"</code>: This line assigns the URL of the file to be downloaded to the <code>url</code> variable.</li>
<li><code>if not os.path.isfile(filename)</code>: This line checks if the file specified by <code>filename</code> does not already exist in the current working directory. The <code>os.path.isfile()</code> function returns <code>True</code> if the file exists and <code>False</code> otherwise.</li>
<li><code>wget.download(url, out=filename)</code>: If the file does not exist, this line uses the <code>wget.download()</code> function to download the file from the specified URL and save it with the name <code>filename</code>. The <code>out</code> parameter is used to specify the output file name.</li>
</ol>
<p>In summary, the code snippet checks if a file with the specified name already exists in the current working directory. If the file does not exist, it downloads the file from the provided URL using the <code>wget</code> library and saves it with the specified filename.</p>
<h1 id="split-the-document-into-chunks">Split the document into chunks<a class="headerlink" href="#split-the-document-into-chunks" title="Permanent link">&para;</a></h1>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.document_loaders</span><span class="w"> </span><span class="kn">import</span> <span class="n">TextLoader</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.text_splitter</span><span class="w"> </span><span class="kn">import</span> <span class="n">CharacterTextSplitter</span>

<span class="n">loader</span> <span class="o">=</span> <span class="n">TextLoader</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="n">documents</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
<span class="n">text_splitter</span> <span class="o">=</span> <span class="n">CharacterTextSplitter</span><span class="o">.</span><span class="n">from_huggingface_tokenizer</span><span class="p">(</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">embeddings_tokenizer</span><span class="p">,</span>
    <span class="n">chunk_size</span><span class="o">=</span><span class="n">embeddings_tokenizer</span><span class="o">.</span><span class="n">max_len_single_sentence</span><span class="p">,</span>
    <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">texts</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
<span class="k">for</span> <span class="n">doc_id</span><span class="p">,</span> <span class="n">text</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">texts</span><span class="p">):</span>
    <span class="n">text</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;doc_id&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">doc_id</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span><span class="si">}</span><span class="s2"> text document chunks created&quot;</span><span class="p">)</span>
</code></pre></div>
<p>This Python script is using the Langchain library to load a text file and split it into smaller chunks. Here's a breakdown of what each part does:</p>
<ol>
<li><code>from langchain.document_loaders import TextLoader</code>: This line imports the TextLoader class from the langchain.document_loaders module. TextLoader is used to load documents from a file.</li>
<li><code>from langchain.text_splitter import CharacterTextSplitter</code> : This line imports the CharacterTextSplitter class from the <code>langchain.text_splitter</code> module. <code>CharacterTextSplitter</code> is used to split text into smaller chunks.</li>
<li><code>loader = TextLoader(filename)</code> : This line creates an instance of <code>TextLoader</code>, which is used to load the text from the specified file <code>(filename)</code>.</li>
<li><code>documents = loader.load()</code> : This line loads the text from the file and stores it in the <code>documents</code> variable as a list of strings.</li>
<li><code>text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(...)</code> : This line creates an instance of <code>CharacterTextSplitter</code>. It takes a Hugging Face tokenizer <code>(embeddings_tokenizer)</code>, sets the chunk size to the maximum length of a single sentence that the tokenizer can handle, and sets the chunk overlap to 0 (meaning no overlap between chunks).</li>
<li><code>texts = text_splitter.split_documents(documents)</code>: This line splits the documents into smaller chunks using the <code>CharacterTextSplitter</code> instance. The result is stored in the texts variable as a list of lists, where each inner list contains the chunks of a single document.</li>
<li><code>for doc_id, text in enumerate(texts): text.metadata["doc_id"] = doc_id</code>: This loop assigns a unique identifier (doc_id) to each chunk of text. The doc_id is the index of the chunk in the texts list.</li>
<li><code>print(f"{len(texts)} text document chunks created")</code>: This line prints the total number of text chunks created.</li>
</ol>
<p>In summary, this script loads a text file, splits it into smaller chunks based on the maximum sentence length that a Hugging Face tokenizer can handle, assigns a unique identifier to each chunk, and then prints the total number of chunks created.</p>
<h2 id="populate-the-vector-database">Populate the vector database<a class="headerlink" href="#populate-the-vector-database" title="Permanent link">&para;</a></h2>
<p>NOTE: Population of the vector database may take over a minute depending on your embedding model and service.</p>
<div class="highlight"><pre><span></span><code><span class="n">ids</span> <span class="o">=</span> <span class="n">vector_db</span><span class="o">.</span><span class="n">add_documents</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span><span class="si">}</span><span class="s2"> documents added to the vector database&quot;</span><span class="p">)</span>
</code></pre></div>
<p>Next we load the <code>texts</code> object created earlier, split it into sentence-sized chunks, and adds these chunks to our vector database, associating each chunk with a unique ID.</p>
<ol>
<li><code>ids = vector_db.add_documents(texts)</code>: This line adds the text chunks to a vector database (<code>vector_db</code>). The <code>add_documents</code> method returns a list of IDs for the added documents.</li>
<li><code>print(f"{len(ids)} documents added to the vector database")</code>: This line prints the number of documents added to the vector database.</li>
</ol>
<h2 id="querying-the-vector-database">Querying the Vector Database<a class="headerlink" href="#querying-the-vector-database" title="Permanent link">&para;</a></h2>
<h3 id="conduct-a-similarity-search">Conduct a similarity search<a class="headerlink" href="#conduct-a-similarity-search" title="Permanent link">&para;</a></h3>
<p>Search the database for similar documents by proximity of the embedded vector in vector space.</p>
<p><div class="highlight"><pre><span></span><code><span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;What did the president say about Ketanji Brown Jackson?&quot;</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">vector_db</span><span class="o">.</span><span class="n">similarity_search</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span><span class="si">}</span><span class="s2"> documents returned&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>  <span class="c1"># Separator for clarity</span>
</code></pre></div>
1. <code>query = "What did the president say about Ketanji Brown Jackson?"</code>: This line assigns the string <code>"What did the president say about Ketanji Brown Jackson?"</code> to the <code>query</code> variable. This is the search query that will be used to find relevant documents in the vector database.
2. <code>docs = vector_db.similarity_search(query)</code>: This line calls the <code>similarity_search()</code> method of the <code>vector_db</code> object, passing the <code>query</code> as an argument. The method returns a list of documents that are most similar to the query based on the vector representations of the documents in the vector database.
3. <code>print(f"{len(docs)} documents returned")</code>: This line prints the number of documents returned by the <code>similarity_search()</code> method. The <code>len()</code> function is used to determine the length of the <code>docs</code> list.
4. <code>for doc in docs:</code>: This line starts a loop that iterates over each document in the <code>docs</code> list.
5. <code>print(doc)</code>: This line prints the content of the current document in the loop.
6. <code>print("=" * 80)</code>: This line prints a separator line consisting of 80 equal signs (<code>=</code>) to improve the readability of the output by visually separating the content of each document.</p>
<p>In summary, the code snippet defines a search query, uses the <code>similarity_search()</code> method of a vector database to find relevant documents, and prints the number of documents returned along with their content. The separator line improves the readability of the output by visually separating the content of each document.</p>
<h2 id="answering-questions">Answering Questions<a class="headerlink" href="#answering-questions" title="Permanent link">&para;</a></h2>
<h3 id="automate-the-rag-pipeline">Automate the RAG pipeline<a class="headerlink" href="#automate-the-rag-pipeline" title="Permanent link">&para;</a></h3>
<p>Build a RAG chain with the model and the document retriever.</p>
<p>First we create the prompts for Granite to perform the RAG query. We use the Granite chat template and supply the placeholder values that the LangChain RAG pipeline will replace.</p>
<p><code>{context}</code> will hold the retrieved chunks, as shown in the previous search, and feeds this to the model as document context for answering our question.</p>
<p>Next, we construct the RAG pipeline by using the Granite prompt templates previously created.</p>
<p><div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.chains.retrieval</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_retrieval_chain</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.chains.combine_documents</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_stuff_documents_chain</span>

<span class="c1"># Create a Granite prompt for question-answering with the retrieved context</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
    <span class="n">conversation</span><span class="o">=</span><span class="p">[{</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;</span><span class="si">{input}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="p">}],</span>
    <span class="n">documents</span><span class="o">=</span><span class="p">[{</span>
        <span class="s2">&quot;title&quot;</span><span class="p">:</span> <span class="s2">&quot;placeholder&quot;</span><span class="p">,</span>
        <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;</span><span class="si">{context}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="p">}],</span>
    <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">prompt_template</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="n">prompt</span><span class="p">)</span>

<span class="c1"># Create a Granite document prompt template to wrap each retrieved document</span>
<span class="n">document_prompt_template</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">Document </span><span class="si">{doc_id}</span>
<span class="si">{page_content}</span><span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>
<span class="n">document_separator</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span>

<span class="c1"># Assemble the retrieval-augmented generation chain</span>
<span class="n">combine_docs_chain</span> <span class="o">=</span> <span class="n">create_stuff_documents_chain</span><span class="p">(</span>
    <span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">prompt</span><span class="o">=</span><span class="n">prompt_template</span><span class="p">,</span>
    <span class="n">document_prompt</span><span class="o">=</span><span class="n">document_prompt_template</span><span class="p">,</span>
    <span class="n">document_separator</span><span class="o">=</span><span class="n">document_separator</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">rag_chain</span> <span class="o">=</span> <span class="n">create_retrieval_chain</span><span class="p">(</span>
    <span class="n">retriever</span><span class="o">=</span><span class="n">vector_db</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(),</span>
    <span class="n">combine_docs_chain</span><span class="o">=</span><span class="n">combine_docs_chain</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>
1. <code>from langchain.prompts import PromptTemplate</code>: This line imports the <code>PromptTemplate</code> class from the <code>langchain.prompts</code> module. This class is used to create custom prompt templates for language models.
2. <code>from langchain.chains.retrieval import create_retrieval_chain</code>: This line imports the <code>create_retrieval_chain()</code> function from the <code>langchain.chains.retrieval</code> module. This function is used to create a retrieval-augmented generation (RAG) chain, which combines a retrieval component (e.g., a vector database) with a language model for generating context-aware responses.
3. <code>from langchain.chains.combine_documents import create_stuff_documents_chain</code>: This line imports the <code>create_stuff_documents_chain()</code> function from the <code>langchain.chains.combine_documents</code> module. This function is used to create a chain that combines multiple retrieved documents into a single input for the language model.
4. <code>prompt = tokenizer.apply_chat_template(...)</code>: This line creates a custom prompt template for a question-answering task using the <code>apply_chat_template()</code> method of the <code>tokenizer</code> object. The prompt template includes a user role with the input question and a document role with the retrieved context. The <code>add_generation_prompt</code> parameter is set to <code>True</code> to include a generation prompt for the language model.
5. <code>prompt_template = PromptTemplate.from_template(template=prompt)</code>: This line creates a <code>PromptTemplate</code> object from the custom prompt template.
6. <code>document_prompt_template = PromptTemplate.from_template(template="""\
Document {doc_id}
{page_content}""")</code>: This line creates a custom prompt template for wrapping each retrieved document. The template includes a document identifier (<code>{doc_id}</code>) and the document content (<code>{page_content}</code>).
7. <code>document_separator="\n\n"</code>: This line assigns the string <code>"\n\n"</code> to the <code>document_separator</code> variable. This separator will be used to separate the content of each retrieved document in the combined input for the language model.
8. <code>combine_docs_chain = create_stuff_documents_chain(...)</code>: This line creates a chain that combines multiple retrieved documents into a single input for the language model using the <code>create_stuff_documents_chain()</code> function. The function takes the language model (<code>model</code>), the prompt template (<code>prompt_template</code>), the document prompt template (<code>document_prompt_template</code>), and the document separator (<code>document_separator</code>) as arguments.
9. <code>rag_chain = create_retrieval_chain(...)</code>: This line creates a retrieval-augmented generation (RAG) chain using the <code>create_retrieval_chain()</code> function. The function takes the retrieval component (i.e., the vector database wrapped with <code>as_retriever()</code>) and the combine documents chain (<code>combine_docs_chain</code>) as arguments.</p>
<p>In summary, the code snippet imports necessary classes and functions from the <code>langchain</code> library to create a retrieval-augmented generation (RAG) chain. It defines a custom prompt template for a question-answering task, creates a document prompt template for wrapping retrieved documents, and assembles the RAG chain by combining the retrieval component and the combine documents chain.</p>
<h2 id="generate-a-retrieval-augmented-response-to-a-question">Generate a retrieval-augmented response to a question<a class="headerlink" href="#generate-a-retrieval-augmented-response-to-a-question" title="Permanent link">&para;</a></h2>
<p>Use the RAG chain to process a question. The document chunks relevant to that question are retrieved and used as context.</p>
<p><div class="highlight"><pre><span></span><code><span class="n">output</span> <span class="o">=</span> <span class="n">rag_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="n">query</span><span class="p">})</span>

<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="s1">&#39;answer&#39;</span><span class="p">])</span>
</code></pre></div>
1. <code>output = rag_chain.invoke({"input": query})</code>: This line invokes the RAG chain with the input query. The <code>invoke()</code> method takes a dictionary as an argument, where the key is <code>"input"</code> and the value is the <code>query</code> string. The method returns a dictionary containing the output of the RAG chain, which includes the generated answer.
2. <code>print(output['answer'])</code>: This line prints the generated answer from the RAG chain output. The <code>output</code> dictionary is accessed using the key <code>'answer'</code>, which corresponds to the generated answer in the RAG chain's response.</p>
<p>In summary, the code snippet invokes the RAG chain with the input query and prints the generated answer from the RAG chain's output.</p>
<h2 id="credits">Credits<a class="headerlink" href="#credits" title="Permanent link">&para;</a></h2>
<p>This notebook is a modified version of the IBM Granite Community <a href="https://github.com/ibm-granite-community/granite-snack-cookbook/blob/main/recipes/RAG/RAG_with_Langchain.ipynb">Retrieval Augmented Generation (RAG) with Langchain</a> notebook. Refer to the <a href="https://github.com/ibm-granite-community">IBM Granite Community</a> for the official notebooks.</p>







  
  






                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../../lab-1/readme/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Lab 1. Summarize a text document with Granite">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Lab 1. Summarize a text document with Granite
              </div>
            </div>
          </a>
        
        
          
          <a href="../../lab-3/readme/" class="md-footer__link md-footer__link--next" aria-label="Next: Lab 3. Energy Demand Forecasting with Granite Timeseries (TTM)">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Lab 3. Energy Demand Forecasting with Granite Timeseries (TTM)
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2024- IBM Research
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/ibm" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://twitter.com/ibmdeveloper" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.linkedin.com/company/ibm/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.youtube.com/user/developerworks" target="_blank" rel="noopener" title="www.youtube.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305m-317.51 213.508V175.185l142.739 81.205z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://dev.to/ibmdeveloper" target="_blank" rel="noopener" title="dev.to" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M120.12 208.29c-3.88-2.9-7.77-4.35-11.65-4.35H91.03v104.47h17.45c3.88 0 7.77-1.45 11.65-4.35s5.82-7.25 5.82-13.06v-69.65c-.01-5.8-1.96-10.16-5.83-13.06M404.1 32H43.9C19.7 32 .06 51.59 0 75.8v360.4C.06 460.41 19.7 480 43.9 480h360.2c24.21 0 43.84-19.59 43.9-43.8V75.8c-.06-24.21-19.7-43.8-43.9-43.8M154.2 291.19c0 18.81-11.61 47.31-48.36 47.25h-46.4V172.98h47.38c35.44 0 47.36 28.46 47.37 47.28zm100.68-88.66H201.6v38.42h32.57v29.57H201.6v38.41h53.29v29.57h-62.18c-11.16.29-20.44-8.53-20.72-19.69V193.7c-.27-11.15 8.56-20.41 19.71-20.69h63.19zm103.64 115.29c-13.2 30.75-36.85 24.63-47.44 0l-38.53-144.8h32.57l29.71 113.72 29.57-113.72h32.58z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.footer", "content.code.copy"], "search": "../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
    
  </body>
</html>